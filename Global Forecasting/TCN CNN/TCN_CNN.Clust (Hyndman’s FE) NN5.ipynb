{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf6089c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T17:15:15.316868Z",
     "iopub.status.busy": "2024-01-03T17:15:15.316548Z",
     "iopub.status.idle": "2024-01-03T17:15:25.575810Z",
     "shell.execute_reply": "2024-01-03T17:15:25.574779Z"
    },
    "papermill": {
     "duration": 10.267736,
     "end_time": "2024-01-03T17:15:25.578039",
     "exception": false,
     "start_time": "2024-01-03T17:15:15.310303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn-extra\r\n",
      "  Downloading scikit_learn_extra-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn-extra) (1.23.5)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn-extra) (1.11.2)\r\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn-extra) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.1.0)\r\n",
      "Installing collected packages: scikit-learn-extra\r\n",
      "Successfully installed scikit-learn-extra-0.3.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "321fcc8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T17:15:25.589387Z",
     "iopub.status.busy": "2024-01-03T17:15:25.589085Z",
     "iopub.status.idle": "2024-01-03T17:15:34.622553Z",
     "shell.execute_reply": "2024-01-03T17:15:34.621600Z"
    },
    "papermill": {
     "duration": 9.041576,
     "end_time": "2024-01-03T17:15:34.624894",
     "exception": false,
     "start_time": "2024-01-03T17:15:25.583318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rstl\r\n",
      "  Downloading rstl-0.1.3-py3-none-any.whl (5.2 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rstl) (1.23.5)\r\n",
      "Installing collected packages: rstl\r\n",
      "Successfully installed rstl-0.1.3\r\n"
     ]
    }
   ],
   "source": [
    "! pip install rstl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ebb6e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T17:15:34.636276Z",
     "iopub.status.busy": "2024-01-03T17:15:34.635984Z",
     "iopub.status.idle": "2024-01-03T17:15:43.896656Z",
     "shell.execute_reply": "2024-01-03T17:15:43.895676Z"
    },
    "papermill": {
     "duration": 9.268576,
     "end_time": "2024-01-03T17:15:43.898762",
     "exception": false,
     "start_time": "2024-01-03T17:15:34.630186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tcn\r\n",
      "  Downloading keras_tcn-3.5.0-py3-none-any.whl (13 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-tcn) (1.23.5)\r\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (from keras-tcn) (2.12.0)\r\n",
      "Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.10/site-packages (from keras-tcn) (0.21.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (23.5.26)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.4.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.2.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.51.3)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (3.9.0)\r\n",
      "Requirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.4.13)\r\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.12.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (16.0.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (21.3)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (3.20.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (68.0.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.16.0)\r\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.12.3)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.12.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (2.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (4.6.3)\r\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (1.14.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-tcn) (0.32.0)\r\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons->keras-tcn) (2.13.3)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->keras-tcn) (0.40.0)\r\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras-tcn) (0.2.0)\r\n",
      "Requirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras-tcn) (1.11.2)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.20.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (1.0.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.4.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.31.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (0.7.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.3.7)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow->keras-tcn) (3.0.9)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (0.2.7)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (4.9)\r\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (1.26.15)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (1.3.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2023.7.22)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (2.1.3)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-tcn) (3.2.2)\r\n",
      "Installing collected packages: keras-tcn\r\n",
      "Successfully installed keras-tcn-3.5.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install keras-tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccd59193",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T17:15:43.912118Z",
     "iopub.status.busy": "2024-01-03T17:15:43.911828Z",
     "iopub.status.idle": "2024-01-03T17:15:51.821178Z",
     "shell.execute_reply": "2024-01-03T17:15:51.820211Z"
    },
    "papermill": {
     "duration": 7.918086,
     "end_time": "2024-01-03T17:15:51.823302",
     "exception": false,
     "start_time": "2024-01-03T17:15:43.905216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tcn import TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2070ea6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-03T17:15:51.836196Z",
     "iopub.status.busy": "2024-01-03T17:15:51.835756Z",
     "iopub.status.idle": "2024-01-03T17:15:52.825273Z",
     "shell.execute_reply": "2024-01-03T17:15:52.824103Z"
    },
    "papermill": {
     "duration": 0.997776,
     "end_time": "2024-01-03T17:15:52.827048",
     "exception": false,
     "start_time": "2024-01-03T17:15:51.829272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# %tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras.models import Sequential,Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import time\n",
    "print(tf.__version__)\n",
    "from keras.layers import MultiHeadAttention\n",
    "from keras.layers import Dense\n",
    "import gc\n",
    "from keras.layers import concatenate\n",
    "import csv\n",
    "import math\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# import xgboost as xgb\n",
    "warnings.filterwarnings('ignore')\n",
    "# import GPy, GPyOpt\n",
    "tfkl = tf.keras.layers\n",
    "tfk = tf.keras\n",
    "\n",
    "from rstl import STL\n",
    "from texttable import Texttable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f02c5ef7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T17:15:52.839813Z",
     "iopub.status.busy": "2024-01-03T17:15:52.839472Z",
     "iopub.status.idle": "2024-01-03T17:15:52.844742Z",
     "shell.execute_reply": "2024-01-03T17:15:52.843925Z"
    },
    "papermill": {
     "duration": 0.013585,
     "end_time": "2024-01-03T17:15:52.846537",
     "exception": false,
     "start_time": "2024-01-03T17:15:52.832952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class Dataset():\n",
    "    \"\"\"\n",
    "    Base class for time series datasets\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Load dataset\n",
    "        must be implemented in subclass\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"you must provide an implementation for load method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "312bdca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T17:15:52.860736Z",
     "iopub.status.busy": "2024-01-03T17:15:52.859263Z",
     "iopub.status.idle": "2024-01-03T17:15:52.865400Z",
     "shell.execute_reply": "2024-01-03T17:15:52.864857Z"
    },
    "papermill": {
     "duration": 0.014226,
     "end_time": "2024-01-03T17:15:52.867046",
     "exception": false,
     "start_time": "2024-01-03T17:15:52.852820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NN5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    NN5 dataset\n",
    "    A Dataset class for loading NN5 time series\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # NN5 path\n",
    "        self.path = '/kaggle/input/nn5-dataset/NN5_DataSet.csv'\n",
    "        # reading NN5 dataset\n",
    "        self.data = np.genfromtxt(self.path, delimiter=',', skip_header=1)\n",
    "        # replace NaN with 0\n",
    "        self.data[np.isnan(self.data)] = 0\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Load dataset\n",
    "        \"\"\"\n",
    "        self.data = self.data.transpose(1, 0)\n",
    "        self.data = pd.DataFrame(self.data)\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "372d776b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T17:15:52.880733Z",
     "iopub.status.busy": "2024-01-03T17:15:52.879964Z",
     "iopub.status.idle": "2024-01-03T17:15:52.900295Z",
     "shell.execute_reply": "2024-01-03T17:15:52.899587Z"
    },
    "papermill": {
     "duration": 0.028914,
     "end_time": "2024-01-03T17:15:52.901946",
     "exception": false,
     "start_time": "2024-01-03T17:15:52.873032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_params(dataset_name = 'hospital'):\n",
    "    suilin_smape = False\n",
    "    \n",
    "\n",
    "    #-------------------------------------------------- NN5 ------------------------------------------------#\n",
    "    if dataset_name == 'nn5':\n",
    "        dataset_path = '/kaggle/working'+'/' + 'NN5'\n",
    "        raw_data = pd.read_csv('/kaggle/input/dataset-nn5-fixing/nn5.csv',sep='delimeter',header=None)\n",
    "\n",
    "        features = pd.read_csv(\"/kaggle/input/hyndmannn5/fs_hyndman_freqfind_nn5.csv\",sep=',', header=0)\n",
    "\n",
    "\n",
    "        lag = 150\n",
    "        look_forward = 56\n",
    "        batch_size = 1\n",
    "        epochs = 1\n",
    "        learning_rate = 0.0001\n",
    "        suilin_smape = False\n",
    "        frequency =7\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    sample_overlap = look_forward - 1\n",
    "\n",
    "\n",
    "    raw_data = raw_data[0].str.split(',', expand=True)\n",
    "\n",
    "    raw_data = raw_data.to_numpy().astype('float64')\n",
    "    features = features.to_numpy().astype('float64')\n",
    "    dataset = []\n",
    "    for i in range(len(raw_data)):\n",
    "        dataset.append(raw_data[i][~np.isnan(raw_data[i])])\n",
    "\n",
    "\n",
    "    return dataset, features, lag, look_forward, sample_overlap, learning_rate, dataset_path, suilin_smape, frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a312f396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T17:15:52.914713Z",
     "iopub.status.busy": "2024-01-03T17:15:52.914257Z",
     "iopub.status.idle": "2024-01-03T17:15:52.925270Z",
     "shell.execute_reply": "2024-01-03T17:15:52.924495Z"
    },
    "papermill": {
     "duration": 0.019066,
     "end_time": "2024-01-03T17:15:52.926923",
     "exception": false,
     "start_time": "2024-01-03T17:15:52.907857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'![root_mean_square_deviation.svg](attachment:root_mean_square_deviation.svg)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_dataset(dataset, look_forward ):\n",
    "    data_means = [];\n",
    "    for index in range(len(dataset)):\n",
    "    # Mean Noramlization\n",
    "        series_mean = np.mean(dataset[index][:len(dataset[index]) - look_forward]) # Train Mean: look_forward || Full Mean: Mean: look_forward = 0\n",
    "\n",
    "        if series_mean == 0:\n",
    "            series_mean = 0.001\n",
    "\n",
    "        data_means.append(series_mean)\n",
    "        dataset[index] = np.divide(dataset[index], series_mean)\n",
    "\n",
    "        # Log Transformation\n",
    "        dataset[index] = np.log(dataset[index] + 1)\n",
    "\n",
    "    return dataset, np.array(data_means)\n",
    "\n",
    "def rescale_data_to_main_value(data, means, dataset_seasonal = []):\n",
    "\n",
    "    for index in range(len(data)):\n",
    "        if len(dataset_seasonal) != 0:\n",
    "            data[index] = data[index] + dataset_seasonal[index]\n",
    "        # Revert Log Transformation\n",
    "        data[index] = np.e ** data[index]\n",
    "        data[index] = data[index] - 1\n",
    "\n",
    "        # Revert Mean Normalization\n",
    "        data[index] = means[index] * data[index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "def normalize_feature_vectors(features):\n",
    "    minimum = features.min(0)\n",
    "    maximum = features.max(0)\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        features[i] = (features[i] - minimum) / (maximum - minimum)\n",
    "\n",
    "    # print(\"R\"*50,minimum)\n",
    "    return features\n",
    "\n",
    "\"\"\"![root_mean_square_deviation.svg](attachment:root_mean_square_deviation.svg)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8976a426",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T17:15:52.940168Z",
     "iopub.status.busy": "2024-01-03T17:15:52.939942Z",
     "iopub.status.idle": "2024-01-03T17:15:52.951340Z",
     "shell.execute_reply": "2024-01-03T17:15:52.950574Z"
    },
    "papermill": {
     "duration": 0.019516,
     "end_time": "2024-01-03T17:15:52.952880",
     "exception": false,
     "start_time": "2024-01-03T17:15:52.933364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#RMSE\n",
    "def root_mean_squared_error(actual, forecast, method = 'single_value'):\n",
    "    # Methods = single_value | per_series\n",
    "    if method == 'single_value':\n",
    "        #Flatten To One Vector\n",
    "        actual = actual.flatten()\n",
    "        forecast = forecast.flatten()\n",
    "\n",
    "        return np.sqrt(np.mean(np.square(actual - forecast)))\n",
    "    elif method == 'per_series':\n",
    "        rmses = []\n",
    "        for i in range(len(actual)):\n",
    "            rmses.append(np.sqrt(np.mean(np.square(actual[i] - forecast[i]))))\n",
    "\n",
    "        return rmses\n",
    "\n",
    "\"\"\"![YIy33.png](attachment:YIy33.png)\"\"\"\n",
    "\n",
    "#SMAPE\n",
    "def single_point_smape(actual, forecast, suilin_smape = False):\n",
    "    if suilin_smape == True:\n",
    "        epsilon = 0.1\n",
    "\n",
    "        return (np.sum(2 * np.abs(forecast - actual) / max((np.abs(actual) + np.abs(forecast))+ epsilon, 0.5 + epsilon)))\n",
    "    else:\n",
    "        return (np.sum(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast))))\n",
    "\n",
    "def smape(actual, forecast, method = 'single_value', suilin_smape = False):\n",
    "    # Methods = single_value | per_series\n",
    "    if method == 'single_value':\n",
    "        #Flatten To One Vector\n",
    "        actual = actual.flatten()\n",
    "        forecast = forecast.flatten()\n",
    "        sum_smape = 0\n",
    "        for i in range(len(actual)):\n",
    "            sum_smape += single_point_smape(actual[i], forecast[i], suilin_smape)\n",
    "        return 100 * sum_smape / len(actual)\n",
    "\n",
    "    elif method == 'per_series':\n",
    "        smapes = []\n",
    "        for i in range(len(actual)):\n",
    "            sum_smape = 0\n",
    "            for j in range(len(actual[i])):\n",
    "                sum_smape += single_point_smape(actual[i,j], forecast[i,j], suilin_smape)\n",
    "            smapes.append(100 * sum_smape / len(actual[i]))\n",
    "        return np.array(smapes)\n",
    "\n",
    "# Create Samples from DataSet\n",
    "def create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal):\n",
    "    if(sample_overlap >= look_forward or sample_overlap < 0): sample_overlap = look_forward - 1\n",
    "    if(look_forward == 1): sample_overlap = 0\n",
    "\n",
    "    dataX, dataY, dataY_seasonal = [], [], []\n",
    "    dataX_means, dataY_means = [], []\n",
    "    for i in range(0, len(sample) - look_back - look_forward+1, look_forward - sample_overlap):\n",
    "        dataX.append(sample[i:(i+look_back), 0])\n",
    "        dataY.append(sample[(i + look_back):(i + look_back + look_forward), 0])\n",
    "\n",
    "        dataY_seasonal.append(dataset_seasonal[(i + look_back):(i + look_back + look_forward)])\n",
    "\n",
    "\n",
    "    return np.array(dataX), np.array(dataY), np.array(dataY_seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c2a2d00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T17:15:52.965435Z",
     "iopub.status.busy": "2024-01-03T17:15:52.965209Z",
     "iopub.status.idle": "2024-01-03T17:15:52.972093Z",
     "shell.execute_reply": "2024-01-03T17:15:52.971331Z"
    },
    "papermill": {
     "duration": 0.015093,
     "end_time": "2024-01-03T17:15:52.973795",
     "exception": false,
     "start_time": "2024-01-03T17:15:52.958702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "testsize = 20/100\n",
    "valsize = 20/100\n",
    "\n",
    "def create_sample(dataX, dataY, data_mean, dataY_seasonal):\n",
    "    test_size=1\n",
    "    val_size=1\n",
    "\n",
    "\n",
    "    train_size=(len(dataX)-test_size)\n",
    "\n",
    "    trainX, testX = dataX[0:train_size,:], dataX[train_size:,:]\n",
    "    trainY, testY = dataY[0:train_size,:], dataY[train_size:,:]\n",
    "\n",
    "    valX, valY = trainX[train_size-val_size:train_size,:],trainY[train_size-val_size:train_size, :]\n",
    "\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0],1, trainX.shape[1]))\n",
    "    valX = np.reshape(valX, (valX.shape[0],1, valX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0],1, testX.shape[1]))\n",
    "\n",
    "    val_means = np.full(len(valY), data_mean)\n",
    "    test_means = np.full(len(testY), data_mean)\n",
    "\n",
    "    val_seasonal = dataY_seasonal[train_size-val_size:train_size, :]\n",
    "    test_seasonal = dataY_seasonal[train_size:,:]\n",
    "\n",
    "    return np.array(trainX),np.array(valX),np.array(testX),np.array(trainY),np.array(valY),np.array(testY), test_means, val_means, val_seasonal, test_seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bda2bbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T17:15:52.986448Z",
     "iopub.status.busy": "2024-01-03T17:15:52.986223Z",
     "iopub.status.idle": "2024-01-03T17:15:53.010504Z",
     "shell.execute_reply": "2024-01-03T17:15:53.009744Z"
    },
    "papermill": {
     "duration": 0.032961,
     "end_time": "2024-01-03T17:15:53.012473",
     "exception": false,
     "start_time": "2024-01-03T17:15:52.979512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess Data For Sampling\n",
    "def all_pre_process(all_dataset, lag, look_forward, sample_overlap, data_means, dataset_seasonal):\n",
    "    look_back = lag\n",
    "\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "\n",
    "    valX = []\n",
    "    valY = []\n",
    "\n",
    "    testX = []\n",
    "    testY = []\n",
    "\n",
    "    all_test_means = []\n",
    "    all_val_means = []\n",
    "\n",
    "    all_test_seasonals = []\n",
    "    all_val_seasonals = []\n",
    "\n",
    "    for index in range(len(all_dataset)):\n",
    "        sample = np.array(all_dataset[index])\n",
    "        sample = sample.reshape(sample.shape[0], 1)\n",
    "\n",
    "        dataX_s, dataY_s, dataY_seasonal = create_dataset(sample, look_back, look_forward, sample_overlap, dataset_seasonal[index])\n",
    "\n",
    "        temp_trainX, temp_valX, temp_testX, temp_trainY, temp_valY, temp_testY, test_means, val_means, val_seasonal, test_seasonal = create_sample(dataX_s,dataY_s,data_means[index], dataY_seasonal)\n",
    "\n",
    "        trainX = trainX + temp_trainX.tolist()\n",
    "        trainY = trainY + temp_trainY.tolist()\n",
    "\n",
    "        valX = valX + temp_valX.tolist()\n",
    "        valY = valY + temp_valY.tolist()\n",
    "\n",
    "        testX = testX + temp_testX.tolist()\n",
    "        testY = testY + temp_testY.tolist()\n",
    "\n",
    "        all_test_means = all_test_means + test_means.tolist()\n",
    "        all_val_means = all_val_means + val_means.tolist()\n",
    "\n",
    "        all_test_seasonals = all_test_seasonals + test_seasonal.tolist()\n",
    "        all_val_seasonals = all_val_seasonals + val_seasonal.tolist()\n",
    "\n",
    "\n",
    "    return np.array(trainX), np.array(valX), np.array(testX), np.array(trainY), np.array(valY), np.array(testY), np.array(all_test_means), np.array(all_val_means), np.array(all_val_seasonals), np.array(all_test_seasonals)\n",
    "\n",
    "def save_prediction_result(data, dataset_name = 'cif-6', dataset_path = ''):\n",
    "    if dataset_name == '':\n",
    "        filename = dataset_name + '-results.csv'\n",
    "    else:\n",
    "        filename = dataset_path + '/' + dataset_name + '-results.csv'\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, sep=',',index=False,header=False)\n",
    "\n",
    "\"\"\"#@main\"\"\"\n",
    "\n",
    "# Main Work & Functionality\n",
    "def run_model_test(dataset, data_means, dataset_seasonal, dataset_name, cluster_lable, lag, look_forward, sample_overlap, batch_size, epochs, learning_rate, suilin_smape, dataset_path, use_saved_model = False, save_trained_model = False):\n",
    "\n",
    "    # Initialize Look Forward & Back\n",
    "    look_back=lag\n",
    "    calculations_method = 'per_series' # single_value | per_series\n",
    "\n",
    "    trainX, valX, testX, trainY, valY, testY, test_means, val_means, val_seasonal, test_seasonal = all_pre_process(dataset, lag, look_forward, sample_overlap, data_means, dataset_seasonal)\n",
    "\n",
    "    # Get Model From Local Saved File\n",
    "    if use_saved_model == True:\n",
    "        if os.path.exists(dataset_path + '/' + dataset_name + '-model-cluster-' + str(cluster_lable)) == True:\n",
    "            model = keras.models.load_model(dataset_path + '/' + dataset_name  + '-model-cluster-' + str(cluster_lable))\n",
    "\n",
    "            val_prediction_results = model.predict([valX],batch_size=16, verbose=0)\n",
    "\n",
    "            val_RMSE = root_mean_squared_error(valY, val_prediction_results, calculations_method)\n",
    "            val_SMAPE = smape(valY, val_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "            ######################################################\n",
    "            test_prediction_results = model.predict([testX],batch_size=16, verbose=0)\n",
    "\n",
    "            test_RMSE = root_mean_squared_error(testY, test_prediction_results, calculations_method)\n",
    "            test_SMAPE = smape(testY, test_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "        else:\n",
    "            use_saved_model = False\n",
    "            save_trained_model = True\n",
    "\n",
    "    # Train Model From Scratch\n",
    "    if use_saved_model == False:\n",
    "\n",
    "        dense_neuron = 100\n",
    "        denselayer_activation = 'linear' #None\n",
    "        output_activation = 'linear' #'linear'\n",
    "\n",
    "        print(\"---------------------------------------------------------------------\")\n",
    "        print(\"lag\", lag)\n",
    "        print(\"look_forward\", look_forward)\n",
    "        print(\"sample overlap\", sample_overlap)\n",
    "        print(\"trainshape\", trainX.shape)\n",
    "        print(\"valshape\", valX.shape)\n",
    "        print(\"testshape\", testX.shape)\n",
    "        print(learning_rate, dense_neuron, denselayer_activation, output_activation)\n",
    "\n",
    "\n",
    "\n",
    "        validation_loss=[]\n",
    "        test_loss=[]\n",
    "        iter = 1\n",
    "        for j in range(iter):\n",
    "### Network Structure\n",
    "            # #---------------------------------------Input Layer------------------------------------------#\n",
    "            input_layer = layers.Input(shape = (1, lag,), name = \"Input-Layer\")\n",
    "\n",
    "\n",
    "\n",
    "            multi_head_attention_layer = TCN(return_sequences=True,dilations=[1, 2, 4, 8])(input_layer)\n",
    "\n",
    "            conv = keras.layers.Conv1D(64,\n",
    "                              strides=2,\n",
    "                              kernel_size=4,\n",
    "                              activation=None,\n",
    "                              padding=\"same\",)(multi_head_attention_layer)\n",
    "            conv2 = keras.layers.Conv1D(16,\n",
    "                              strides=2,\n",
    "                              kernel_size=4,\n",
    "                              activation=None,\n",
    "                              padding=\"same\",)(conv)\n",
    "            flatten_layer2=keras.layers.Flatten(name=\"Flatten-Layer2\")(conv2)\n",
    "            # concat=keras.layers.concatenate([flatten_layer1,flatten_layer2])\n",
    "\n",
    "            dense_layer1 = Dense(\n",
    "                dense_neuron,\n",
    "                activation = denselayer_activation,\n",
    "                name = \"Fully-Connected-Layer\")(flatten_layer2)\n",
    "\n",
    "            dense_layer2 = Dense(\n",
    "                look_forward,\n",
    "                activation = None,\n",
    "                name = \"Output-Layer\")(dense_layer1)\n",
    "\n",
    "\n",
    "            # Create Model\n",
    "            model = Model(inputs = [input_layer], outputs = dense_layer2)\n",
    "\n",
    "            # Optimizer\n",
    "            opt=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "            # Compile Model\n",
    "            model.compile(loss=\"mse\",optimizer=opt,metrics=[\"mse\"])\n",
    "\n",
    "            for k in range(epochs):\n",
    "                history = model.fit([trainX], trainY, validation_data=([valX, valY]),\n",
    "                    verbose = 0,\n",
    "                    batch_size = batch_size,\n",
    "        #             callbacks = [\n",
    "        #                 tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=20, restore_best_weights=True),\n",
    "        #                 tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=20, factor=0.5, min_lr=1e-5)\n",
    "        #             ]\n",
    "            ).history\n",
    "\n",
    "                val_prediction_results = model.predict([valX],batch_size=16, verbose=0)\n",
    "\n",
    "                val_RMSE = root_mean_squared_error(valY, val_prediction_results, calculations_method)\n",
    "                val_SMAPE = smape(valY, val_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "                ######################################################\n",
    "                test_prediction_results = model.predict([testX],batch_size=16, verbose=0)\n",
    "\n",
    "                test_RMSE = root_mean_squared_error(testY, test_prediction_results, calculations_method)\n",
    "                test_SMAPE = smape(testY, test_prediction_results, calculations_method, suilin_smape)\n",
    "\n",
    "                validation_loss.append(np.mean(val_RMSE))\n",
    "                test_loss.append(np.mean(test_RMSE))\n",
    "\n",
    "            K.clear_session()\n",
    "            gc.collect()\n",
    "\n",
    "            # Save model to a file if wanted\n",
    "            if save_trained_model == True:\n",
    "                model.save(dataset_path + '/' + dataset_name + '-model-cluster-' + str(cluster_lable))\n",
    "\n",
    "            model=None\n",
    "            del history\n",
    "\n",
    "    #############################################\n",
    "    rescaled_valY = rescale_data_to_main_value(valY, val_means, val_seasonal)\n",
    "    rescaled_val_prediction_results = rescale_data_to_main_value(val_prediction_results, val_means)\n",
    "    val_SMAPE = smape(rescaled_valY, rescaled_val_prediction_results, calculations_method, suilin_smape)\n",
    "    val_RMSE = root_mean_squared_error(rescaled_valY, rescaled_val_prediction_results, calculations_method)\n",
    "\n",
    "\n",
    "    rescaled_testY = rescale_data_to_main_value(testY, test_means, test_seasonal)\n",
    "    rescaled_test_prediction_results = rescale_data_to_main_value(test_prediction_results, test_means, test_seasonal)\n",
    "    test_SMAPE = smape(rescaled_testY, rescaled_test_prediction_results, calculations_method)\n",
    "    test_RMSE = root_mean_squared_error(rescaled_testY, rescaled_test_prediction_results, calculations_method)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # print(\"TestY\",rescaled_testY)\n",
    "    # print( \"Prediction\",rescaled_test_prediction_results)\n",
    "\n",
    "\n",
    "    results = {\n",
    "            'val_SMAPE': val_SMAPE,\n",
    "            'val_RMSE': val_RMSE,\n",
    "            'test_SMAPE': test_SMAPE,\n",
    "            'test_RMSE': test_RMSE\n",
    "        }\n",
    "    #############################################\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9814617f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T17:15:53.025762Z",
     "iopub.status.busy": "2024-01-03T17:15:53.025447Z",
     "iopub.status.idle": "2024-01-03T17:15:53.040658Z",
     "shell.execute_reply": "2024-01-03T17:15:53.040083Z"
    },
    "papermill": {
     "duration": 0.02341,
     "end_time": "2024-01-03T17:15:53.042170",
     "exception": false,
     "start_time": "2024-01-03T17:15:53.018760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def cluster_series(features, number_of_clusters=2):\n",
    "    clustered = KMedoids(n_clusters=number_of_clusters, init='k-medoids++',random_state=0).fit(features) # Kmedoids init='k-medoids++'\n",
    "    print(\"Kmedoids\")\n",
    "\n",
    "    print('silhouette_score -------->', silhouette_score(features, clustered.labels_))\n",
    "    return clustered.labels_\n",
    "\n",
    "def stl_decomposition(dataset, frequency):\n",
    "    seasonal = []\n",
    "    trend = []\n",
    "    for index in range(len(dataset)):\n",
    "        if frequency != None:\n",
    "            stl = STL(dataset[index], frequency, \"periodic\")\n",
    "\n",
    "            seasonal.append(stl.seasonal)\n",
    "            trend.append(stl.trend)\n",
    "            dataset[index] = dataset[index] - stl.seasonal\n",
    "        else:\n",
    "            seasonal.append(np.zeros((dataset[index].shape)))\n",
    "            trend.append(np.zeros((dataset[index].shape)))\n",
    "\n",
    "    return dataset, np.array(seasonal), np.array(trend)\n",
    "\n",
    "def run_local_models(dataset_name, number_of_clusters=2, AEName='LSTM', Dim=8, epochs = 20, batch =20, use_saved_model = False, save_trained_model = False, run=1):\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print('dataset: ', dataset_name)\n",
    "    batch_size = batch\n",
    "    epochs = epochs\n",
    "    # Prepare & Read Data\n",
    "    dataset, features, lag, look_forward, sample_overlap, learning_rate, dataset_path, suilin_smape, frequency = get_dataset_params(dataset_name)\n",
    "\n",
    "    # Normalize Data\n",
    "    dataset, data_means = normalize_dataset(dataset, look_forward = 0)\n",
    "\n",
    "    dataset, seasonal, trend = stl_decomposition(dataset, frequency)\n",
    "\n",
    "    # Normalize Features\n",
    "    features = normalize_feature_vectors(features)\n",
    "\n",
    "    # Cluster Series Based On Feature Vectors (Feature Based Clustering)\n",
    "    if number_of_clusters == 1:\n",
    "        clusters = np.zeros(len(features))\n",
    "    else:\n",
    "        clusters = cluster_series(features, number_of_clusters)\n",
    "\n",
    "    dataset = np.array(dataset)\n",
    "\n",
    "    results = {\n",
    "        'val_SMAPE': np.array([]),\n",
    "        'val_RMSE': np.array([]),\n",
    "        'test_SMAPE': np.array([]),\n",
    "        'test_RMSE': np.array([])\n",
    "    }\n",
    "\n",
    "    # Loop Trough Clusters\n",
    "    for cluster_lable in range(number_of_clusters):\n",
    "        idx = [x for x in range(len(clusters)) if clusters[x] == cluster_lable]\n",
    "        cluster_dataset = np.array(dataset)[idx]\n",
    "        cluster_dataset_means = data_means[idx]\n",
    "        cluster_dataset_seasonal = seasonal[idx]\n",
    "\n",
    "        result = run_model_test(cluster_dataset, cluster_dataset_means, cluster_dataset_seasonal, dataset_name, cluster_lable, lag, look_forward, sample_overlap, batch_size, epochs, learning_rate, suilin_smape, dataset_path, use_saved_model, save_trained_model)\n",
    "\n",
    "        results = {\n",
    "            'val_SMAPE': np.concatenate((results['val_SMAPE'], result['val_SMAPE'])),\n",
    "            'val_RMSE': np.concatenate((results['val_RMSE'], result['val_RMSE'])),\n",
    "            'test_SMAPE': np.concatenate((results['test_SMAPE'], result['test_SMAPE'])),\n",
    "            'test_RMSE': np.concatenate((results['test_RMSE'], result['test_RMSE'])),\n",
    "        }\n",
    "    # print(\"test smape\",results['test_SMAPE'])\n",
    "    # Print Results Table\n",
    "    t = Texttable()\n",
    "    print('\\n\\n#------------------------------------Scaled------------------------------------#')\n",
    "    t.add_rows([\n",
    "        ['Index', 'Mean sMAPE', 'Median sMAPE', 'Mean RMSE', 'Median RMSE'],\n",
    "        ['Validate', np.mean(results['val_SMAPE']), np.median(results['val_SMAPE']), np.mean(results['val_RMSE']), np.median(results['val_RMSE'])],\n",
    "        ['Test', np.mean(results['test_SMAPE']), np.median(results['test_SMAPE']), np.mean(results['test_RMSE']), np.median(results['test_RMSE'])]\n",
    "    ])\n",
    "    \n",
    "    print(t.draw())\n",
    "    t = Texttable()\n",
    "    t.add_rows([\n",
    "        ['Run', 'N.Epochs', 'Batch Size', 'Auto Encoder', 'Latent Dimension'],\n",
    "        [run, epochs,batch_size, AEName, Dim]\n",
    "    ])\n",
    "    print(t.draw())\n",
    "    initial_data = [\n",
    "    ['Run', 'N.Epochs', 'Batch Size', 'Auto Encoder', 'Latent Dimension', 'Index', 'Mean sMAPE', 'Median sMAPE', 'Mean RMSE', 'Median RMSE'],\n",
    "    [run, epochs, batch_size, AEName, Dim, 'Validate', np.mean(results['val_SMAPE']), np.median(results['val_SMAPE']), np.mean(results['val_RMSE']), np.median(results['val_RMSE'])],\n",
    "    [run, epochs, batch_size, AEName, Dim, 'Test', np.mean(results['test_SMAPE']), np.median(results['test_SMAPE']), np.mean(results['test_RMSE']), np.median(results['test_RMSE'])]\n",
    "    ]# Write the header and initial data to the CSV file\n",
    "\n",
    "    return results, initial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2755ba64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T17:15:53.054360Z",
     "iopub.status.busy": "2024-01-03T17:15:53.054147Z",
     "iopub.status.idle": "2024-01-03T17:15:53.075656Z",
     "shell.execute_reply": "2024-01-03T17:15:53.074591Z"
    },
    "papermill": {
     "duration": 0.029445,
     "end_time": "2024-01-03T17:15:53.077343",
     "exception": false,
     "start_time": "2024-01-03T17:15:53.047898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "five_run= {\n",
    "        'mean_val_SMAPE_5': np.array([]),\n",
    "        'mean_val_RMSE_5': np.array([]),\n",
    "        'mean_test_SMAPE_5': np.array([]),\n",
    "        'mean_test_RMSE_5': np.array([]),\n",
    "        'median_val_SMAPE_5': np.array([]),\n",
    "        'median_val_RMSE_5': np.array([]),\n",
    "        'median_test_SMAPE_5': np.array([]),\n",
    "        'median_test_RMSE_5': np.array([])\n",
    "    }\n",
    " \n",
    "def nn5(AEName=\"Hyndman\", Dim=0, run=1):\n",
    "    \n",
    "    ds_names = [AEName, [50], [20, 40, 60, 80, 100], Dim]\n",
    "    file_mode = 'w'\n",
    "    csv_file = \"results_\"+AEName+\"_\"+str(Dim)+\".csv\"\n",
    "    if os.path.exists(csv_file):\n",
    "        file_mode = 'a'\n",
    "    else:\n",
    "        file_mode = 'w'\n",
    "\n",
    "    with open(csv_file, mode=file_mode, newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for epoch in ds_names[1]:\n",
    "            for batch in ds_names[2]:\n",
    "    \n",
    "                hospital_results, initial_data = run_local_models(dataset_name = 'nn5', number_of_clusters = 2, AEName=ds_names[0], Dim=ds_names[3], epochs = epoch, batch = batch, use_saved_model = False, save_trained_model = False, run=run)\n",
    "                writer.writerows(initial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a2743ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T17:15:53.089733Z",
     "iopub.status.busy": "2024-01-03T17:15:53.089465Z",
     "iopub.status.idle": "2024-01-03T21:08:24.321239Z",
     "shell.execute_reply": "2024-01-03T21:08:24.319945Z"
    },
    "papermill": {
     "duration": 13951.253129,
     "end_time": "2024-01-03T21:08:24.336109",
     "exception": false,
     "start_time": "2024-01-03T17:15:53.082980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.161     | 18.429       | 4.542     | 4.327       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 19.938     | 19.118       | 4.741     | 4.425       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 0   | 50       | 20         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.773     | 18.820       | 4.649     | 4.429       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.374     | 19.491       | 4.811     | 4.554       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 0   | 50       | 40         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.581     | 18.814       | 4.612     | 4.307       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.666     | 19.616       | 4.857     | 4.560       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 0   | 50       | 60         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 20.034     | 19.424       | 4.722     | 4.448       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.490     | 19.577       | 4.855     | 4.611       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 0   | 50       | 80         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 20.065     | 19.329       | 4.712     | 4.444       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.081     | 19.255       | 4.776     | 4.467       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 0   | 50       | 100        | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "**************************************************\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.062     | 18.601       | 4.500     | 4.175       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.498     | 19.575       | 4.867     | 4.516       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 1   | 50       | 20         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.881     | 19.062       | 4.690     | 4.429       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.225     | 19.631       | 4.785     | 4.479       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 1   | 50       | 40         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.492     | 18.948       | 4.626     | 4.357       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.412     | 19.772       | 4.826     | 4.490       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 1   | 50       | 60         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 20.088     | 19.422       | 4.725     | 4.452       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.174     | 19.407       | 4.795     | 4.524       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 1   | 50       | 80         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.893     | 18.751       | 4.691     | 4.439       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.440     | 19.682       | 4.834     | 4.496       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 1   | 50       | 100        | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "**************************************************\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.327     | 18.490       | 4.549     | 4.253       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 19.665     | 18.724       | 4.702     | 4.435       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 2   | 50       | 20         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.918     | 19.193       | 4.688     | 4.419       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.132     | 19.500       | 4.777     | 4.415       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 2   | 50       | 40         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 20.387     | 19.480       | 4.788     | 4.568       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.637     | 19.884       | 4.863     | 4.618       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 2   | 50       | 60         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.463     | 18.697       | 4.606     | 4.401       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 19.931     | 19.027       | 4.757     | 4.405       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 2   | 50       | 80         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 20.081     | 19.240       | 4.732     | 4.403       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.145     | 19.132       | 4.792     | 4.442       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 2   | 50       | 100        | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "**************************************************\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.029     | 18.544       | 4.536     | 4.276       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.201     | 19.444       | 4.809     | 4.473       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 3   | 50       | 20         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.514     | 18.716       | 4.650     | 4.352       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.263     | 19.441       | 4.810     | 4.495       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 3   | 50       | 40         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.959     | 19.168       | 4.717     | 4.447       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 19.641     | 19.071       | 4.670     | 4.356       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 3   | 50       | 60         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.391     | 18.736       | 4.544     | 4.193       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.611     | 19.722       | 4.864     | 4.626       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 3   | 50       | 80         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 20.420     | 19.520       | 4.796     | 4.576       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.325     | 19.178       | 4.838     | 4.571       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 3   | 50       | 100        | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "**************************************************\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.407     | 18.699       | 4.584     | 4.238       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 19.914     | 19.125       | 4.749     | 4.478       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 4   | 50       | 20         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.434     | 18.569       | 4.590     | 4.349       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.283     | 19.299       | 4.823     | 4.615       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 4   | 50       | 40         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 20.063     | 19.752       | 4.713     | 4.498       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.159     | 19.676       | 4.776     | 4.423       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 4   | 50       | 60         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 19.686     | 18.986       | 4.661     | 4.405       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 20.121     | 19.115       | 4.789     | 4.553       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 4   | 50       | 80         | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "dataset:  nn5\n",
      "Kmedoids\n",
      "silhouette_score --------> 0.24171679857030026\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (52650, 1, 150)\n",
      "valshape (90, 1, 150)\n",
      "testshape (90, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "---------------------------------------------------------------------\n",
      "lag 150\n",
      "look_forward 56\n",
      "sample overlap 55\n",
      "trainshape (12285, 1, 150)\n",
      "valshape (21, 1, 150)\n",
      "testshape (21, 1, 150)\n",
      "0.0001 100 linear linear\n",
      "\n",
      "\n",
      "#------------------------------------Scaled------------------------------------#\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "|  Index   | Mean sMAPE | Median sMAPE | Mean RMSE | Median RMSE |\n",
      "+==========+============+==============+===========+=============+\n",
      "| Validate | 20.051     | 19.750       | 4.719     | 4.379       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "| Test     | 21.135     | 20.154       | 4.950     | 4.703       |\n",
      "+----------+------------+--------------+-----------+-------------+\n",
      "+-----+----------+------------+--------------+------------------+\n",
      "| Run | N.Epochs | Batch Size | Auto Encoder | Latent Dimension |\n",
      "+=====+==========+============+==============+==================+\n",
      "| 4   | 50       | 100        | Hyndman      | 0                |\n",
      "+-----+----------+------------+--------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"*\" * 50)\n",
    "    nn5(run = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92128e2",
   "metadata": {
    "papermill": {
     "duration": 0.012633,
     "end_time": "2024-01-03T21:08:24.361875",
     "exception": false,
     "start_time": "2024-01-03T21:08:24.349242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3743351,
     "sourceId": 6479400,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3785054,
     "sourceId": 6548814,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3792584,
     "sourceId": 6564428,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3863893,
     "sourceId": 6704564,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3949919,
     "sourceId": 6873766,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 157337232,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 157337836,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 157344405,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13993.52911,
   "end_time": "2024-01-03T21:08:25.917367",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-03T17:15:12.388257",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
